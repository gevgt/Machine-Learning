import numpy as np
from scipy.stats import rankdata
from sklearn import datasets
from matplotlib import pyplot as plt

"""
This library includes:
    - Clustering:                                   ml.clustering(data, n_groups, lap=10)
    - K-Nearest_Neighbors with Cross Validation:    ml.knn_cv(data, fold=5, K=10)

To call a function
import [name of script] as ml
and just copy the certain function written above!

Furthermore, there is an example beneath each function. Just copy and paste ;D
"""



import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt


    
def clustering(data, n_groups, laps=10):
    d_sum_max = np.max(data) * 10000
    
    for lap in range(laps):
        #Random allocation of groups
        group = np.random.randint(n_groups, size = data.shape[0])
        
        #Makes sure to pass the first reallocation process
        first = True
        
        while True:    
            #Skips this part in the first loop, since a mean of the groups is required
            if first == False:
                
                #Reallocates the observations to the group to which mean they are closest to
                comp = np.copy(group)
                for i in range(data.shape[0]):
                    distance = np.max(data) * 10000
                    for j in range(n_groups):     
                        if np.sum((data[i]-mean[j])**2) < distance:
                            distance = np.sum((data[i]-mean[j])**2)
                            group[i] = j    
            
            #Calculation of the groups mean    
            mean = np.ones((1,data.shape[1]))
            for i in range(n_groups):
                v_summe = np.ones((1,data.shape[1]))
                n = 0
                for j in range(data.shape[0]):
                    if group[j] == i:
                        v_summe = v_summe + data[j]
                        n += 1
                v_mean = v_summe / n
                mean = np.concatenate((mean, v_mean.reshape(1,data.shape[1])), axis=0)
            mean = mean[1:]
            
            #Breaks the loop if the observations do not change the groups anymore
            if first == False:    
                if np.sum(comp-group) == 0:
                    break
                
            #Enables the reallocation after the first loop
            first = False
        
        #Aggregated sum of distance between observation and groups mean
        d_sum = 0
        for i in range(data.shape[0]):
            d_sum += np.sum((data[i]-mean[group[i]])**2)
        
        #Saves the group allocation with the smallest aggregated distance
        if d_sum < d_sum_max:
            d_sum_max = d_sum
            group_final = np.copy(group)
            mean_final = np.copy(mean)
    
    #Plots a chart if the dataset is 2D
    if data.shape[1] == 2:
        plt.figure()
        for i in range(n_groups):
            plot_data = np.ones((1, data.shape[1]))
            for j in range(group_final.shape[0]):
                if group_final[j] == i:
                    plot_data = np.concatenate((plot_data, data[j].reshape(1,2)), axis=0)
            plot_data = plot_data[1:]
            plt.scatter(plot_data[:,0], plot_data[:,1])
        
        for i in range(n_groups):
            plt.scatter(mean_final[i,0], mean_final[i,1], linewidth=3, color="red")
        plt.show()
        
    return group_final, d_sum
        
        
"""
#Iris Example:       
iris = datasets.load_iris()
data = iris["data"][:,:2]

clustering(data, n_groups=3)"""    





def knn_cv(data, fold=5, K=10):
    print("Fold = " + str(fold))
    max_er = 1                          
    
    #K different values for k to find the k with the least smalles error rate
    for k in range(1,K+1):
        set_len = int(data.shape[0] / fold)
        fehler = 0
        for i in range(set_len,data.shape[0]+1, set_len):
            #Creating a test and training sample from the original data set
            test_data = data[i-set_len:i]
            training_data = np.delete(data, np.s_[i-set_len:i],0)
            
            #For each datapoint in the test sample
            for j in range(set_len):
                #Calculating the Frobenius Norm to each training observation
                distance = np.sum((training_data[:,:-1] - test_data[j,:-1])**2, axis=1)
                
                #Ranks the Frobenius Norms to find the k-smallest distances
                rank = rankdata(distance, method="ordinal")
                
                n_classes = np.array([])
                for k_2 in range(k):
                    for l in range(rank.shape[0]):
                        if rank[l] == k_2+1:
                            n_classes = np.append(n_classes, training_data[l,-1])
                
                #Determine the most frequent class within the KNN
                count = np.bincount(np.array(n_classes, dtype=int))
                
                max = -1
                for m in range(count.shape[0]):
                    if count[m] > max:
                        max = count[m]
                        decision = m
                
                #Error Rate
                if decision != test_data[j, -1]:
                    fehler += 1
        
        d_fehler = fehler / data.shape[0]
        if d_fehler < max_er:
            rec_k = k
            max_er = d_fehler
        
        print("Error Rate for " + str(k) + "-NN: " + str(round(d_fehler*100,1)) + " %")
    print("Recommended K: " + str(rec_k))
        
        
"""
Example:
iris = datasets.load_iris()
data = iris["data"][:,:2]
klasse = iris["target"]

data = np.concatenate((data, klasse.reshape(klasse.shape[0],1)), axis=1) 

ml.knn_cv(data, fold=5)
"""
